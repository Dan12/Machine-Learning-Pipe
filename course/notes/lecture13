Ups and downs of backpropegation
    -backprop invented in '69 by bryson and Ho, linear version
    -backprop invented several times
    -backprop had mucb promise but it didn't live up
        -still widely use by psycologist
    -couldn't make good use of multiple hidden layers
    -didn't work well in recurrent nets or deep autoencoders
    -support vector machines worked better
    -really failed because computers were slow and data sets were too small
    -spectrum of ML
        -statistics: low dimensional data (100 dimensions), lots of noise, not much structure in the data, trying to separate true structure from noise
        -Artificial intelliegence: high dim data, noise is not the main problem, lots of structure that is very complicated
    -Support vector machines
        -expand the input to a very large layer of non linear non adaptive features, one layer of adaptive weights, efficient control of overfitting
        -can't learn multiple layers of representation with SVM
Belief Nets (link: https://share.coursera.org/wiki/index.php/Lecture_13b_-_The_math_of_Sigmoid_Belief_Networks)
    -deep networks, learning time does not scale well, requires lots of labeled data, can get stuck in poor local optima
    -use unsupervised learning to overcome some of the limitations of backprop
    -use the method for modeling the structure of the input, not for the relationship between the input and output
    -adjust weights to maximize the probability that a generative model would have generated the input
        -maximize p(x), not p(x|y)
    -could use energy based model or a causal model of idealized neurons, or hybrid of the two
    -probability: problems had to deal with uncertainty, graphical models, probability used to compute the states of nodes given the states of other nodes
    -belief nets: sparsely connected direct acyclic graphs, clever inference algorithms
    -inference problem: infer the states of unobserved variables
    -learning problem: adjust the interactions to make the networks mre likely to generate the training data
    -energy and causal models (sigmoid belief net)
Learning Sigmoid Belief Nets
    -easy to generate unbiased samples at the leaf nodes (see what the network believes in)
    -hard to infer the posterior distribution over all possible configurations of the hidden causes (exponential of hidden nodes)
    -learning rule: if we can get an unbiased sample from the distribution over hidden states given the data, maximize the log probability that the binary state would be generated by the sampled parents
    -delta wji = e*sj*(si-pi)
    -hard to sample from the posterior (unbiased)
        -explaining away: get an anti correclation between the two hidden causes, XOR, two paterns of causes which are the oposites of each other
    -monte carlo method: slow in large deep belief nets
    -variational method, approximate samples from the posterior
Wake-sleep algorithm
    -using an approximate posterior, hope that learning still works
    -factorial distribution
        -probability of a whole vector is the product of the probabilities of its individual terms
        -p(on) = .3,.6,.8 so p(1,0,1) = .3*(1-.6)*.8
    -wake sleep algorithm
        -wake phase: use recognition weights to perform a forward pass with data, make a stochastic binary decision at each hidden weight
            -learn the generative weights with maximum likelyhood learnign
        -sleep phase: use generative weights to generate samples from the model, generate unbiased sample
            -learn the recognition weights
        -mode averaging: better to pick just one mode
        